## BUILD DOCKER CONTAINER

docker build --network=host -t pauldefusco/dex-spark-runtime-3.2.3-7.2.15.8:1.20.0-b15-dbldatagen-002 . -f Dockerfile

docker run -it pauldefusco/dex-spark-runtime-3.2.3-7.2.15.8:1.20.0-b15-dbldatagen-002 /bin/bash

docker push pauldefusco/dex-spark-runtime-3.2.3-7.2.15.8:1.20.0-b15-dbldatagen-002

cde credential create --name docker-creds --type docker-basic --docker-server hub.docker.com --docker-username pauldefusco

## CREATE DOCKER RUNTIME RESOURCE

cde resource create --name dex-spark-runtime-dbldatagen-002 --image pauldefusco/dex-spark-runtime-3.2.3-7.2.15.8:1.20.0-b15-dbldatagen-002 --image-engine spark3 --type custom-runtime-image

#cde spark submit --user=pauldefusco CDE_Runtime/dbldatagen_example.py --runtime-image-resource-name=dex-spark-runtime-dbldatagen-002

## CREATE FILE RESOURCE

cde resource create --name sparkgenfiles

cde resource upload --name sparkgenfiles --local-path CDE_Runtime/datagen.py

cde resource upload --name sparkgenfiles --local-path CDE_Runtime/parameters.conf

## CREATE SPARKGEN METADATA TABLES

cde resource upload --name sparkgenfiles --local-path CDE_Runtime/table_gen.py

cde job create --name tablegen --type spark --mount-1-resource sparkgenfiles --application-file table_gen.py --runtime-image-resource-name dex-spark-runtime-dbldatagen-002

cde job run --name tablegen

## CREATE TARGET TABLE AND INSERT METADATA INTO SPARKGEN METADATA TABLES

cde resource upload --name sparkgenfiles --local-path CDE_Runtime/cde_autogen_target.py

cde job create --name sparkgen_job_target --type spark --mount-1-resource sparkgenfiles --application-file cde_autogen_target.py --executor-memory "4g" --executor-cores 4 --user pauldefusco --runtime-image-resource-name dex-spark-runtime-dbldatagen-002

cde job run --name sparkgen_job_target

## CREATE STAGING AND MERGE INTO JOBS WITH ORCH DAG AND RUN ORCH DAG

cde resource upload --name sparkgenfiles --local-path CDE_Runtime/cde_autogen_staging.py

cde resource upload --name sparkgenfiles --local-path CDE_Runtime/cde_mergeinto.py

cde resource upload --name sparkgenfiles --local-path CDE_Runtime/airflow_DAG.py

cde resource upload --name sparkgenfiles --local-path CDE_Runtime/iceberg_metadata_queries.py

cde resource upload --name sparkgenfiles --local-path CDE_Runtime/spark-measure_2.12-0.23.jar

cde resource create --type python-env --name sparkgen_py

cde resource upload --local-path CDE_Runtime/requirements.txt --name sparkgen_py

cde job create --name sparkgen_job_staging --type spark --mount-1-resource sparkgenfiles --application-file cde_autogen_staging.py --jar spark-measure_2.12-0.23.jar --executor-memory "4g" --executor-cores 4 --user pauldefusco --runtime-image-resource-name dex-spark-runtime-dbldatagen-002 --arg {{{RUN_ID}}}

cde job create --name sparkgen_mergeinto --type spark --mount-1-resource sparkgenfiles --application-file cde_mergeinto.py --jar spark-measure_2.12-0.23.jar --python-env-resource-name sparkgen_py --executor-memory "4g" --executor-cores 4 --user pauldefusco --arg {{{RUN_ID}}}

cde job create --name sparkgen_orch --type airflow --mount-1-resource sparkgenfiles --dag-file airflow_DAG.py

cde job create --name iceberg_metadata_queries --type spark --mount-1-resource sparkgenfiles --application-file iceberg_metadata_queries.py

cde job run --name iceberg_metadata_queries

#cde job run --name sparkgen_orch

## UPDATE DAG

cde resource upload --name sparkgenfiles --local-path CDE_Runtime/airflow_DAG.py

cde job update --name sparkgen_orch --mount-1-resource sparkgenfiles --dag-file airflow_DAG.py

cde job run --name sparkgen_orch

## RUN SINGLE JOB

cde job run --name sparkgen_mergeinto --arg 12345
